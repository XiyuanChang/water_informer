{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y = []\n",
    "for i in range(1, 291):\n",
    "    file = f\"Maumee DL/{i}_climate_sediment.csv\"\n",
    "    with open(file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    array = df['sedyld'].tolist()\n",
    "    y.extend(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array(y)\n",
    "pred = np.zeros_like(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_nse(observed, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Nash-Sutcliffe Efficiency (NSE) coefficient.\n",
    "    \n",
    "    Parameters:\n",
    "    observed (numpy.ndarray): An array of observed values.\n",
    "    predicted (numpy.ndarray): An array of predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    float: The NSE coefficient.\n",
    "    \"\"\"\n",
    "    if observed.shape != predicted.shape:\n",
    "        raise ValueError(\"The shape of observed and predicted arrays must be the same.\")\n",
    "\n",
    "    # Calculate the mean of the observed data\n",
    "    mean_observed = np.mean(observed)\n",
    "    \n",
    "    # Calculate the numerator of the fraction (residual sum of squares)\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    \n",
    "    # Calculate the denominator of the fraction (total sum of squares)\n",
    "    denominator = np.sum((observed - mean_observed) ** 2)\n",
    "    \n",
    "    # Calculate the NSE\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    \n",
    "    return nse\n",
    "print(calculate_nse(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared(observed, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the coefficient of determination, R^2.\n",
    "    \n",
    "    Parameters:\n",
    "    observed (numpy.ndarray): An array of observed values.\n",
    "    predicted (numpy.ndarray): An array of predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    float: The R^2 value.\n",
    "    \"\"\"\n",
    "    if observed.shape != predicted.shape:\n",
    "        raise ValueError(\"The shape of observed and predicted arrays must be the same.\")\n",
    "\n",
    "    # Calculate the mean of the observed data\n",
    "    mean_observed = np.mean(observed)\n",
    "    # Calculate the mean of the predicted data\n",
    "    mean_predicted = np.mean(predicted)\n",
    "    \n",
    "    # Calculate the numerator of the R^2 formula\n",
    "    numerator = np.sum((observed - mean_observed) * (predicted - mean_predicted)) ** 2\n",
    "    \n",
    "    # Calculate the denominator of the R^2 formula\n",
    "    denominator = (np.sum((observed - mean_observed) ** 2) *\n",
    "                   np.sum((predicted - mean_predicted) ** 2))\n",
    "    \n",
    "    # Calculate the R^2 value\n",
    "    r_squared = numerator / denominator\n",
    "    \n",
    "    return r_squared\n",
    "print(calculate_r_squared(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pbias(observed, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Percent Bias (PBIAS).\n",
    "    \n",
    "    Parameters:\n",
    "    observed (numpy.ndarray): An array of observed values.\n",
    "    predicted (numpy.ndarray): An array of predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    float: The PBIAS value.\n",
    "    \"\"\"\n",
    "    if observed.shape != predicted.shape:\n",
    "        raise ValueError(\"The shape of observed and predicted arrays must be the same.\")\n",
    "    \n",
    "    # Calculate the numerator of the PBIAS formula\n",
    "    numerator = np.sum(observed - predicted)\n",
    "    \n",
    "    # Calculate the denominator of the PBIAS formula\n",
    "    denominator = np.sum(observed)\n",
    "    \n",
    "    # Calculate the PBIAS\n",
    "    pbias = (numerator / denominator) * 100\n",
    "    \n",
    "    return pbias\n",
    "print(calculate_pbias(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Maumee DL/276_climate_sediment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station = pd.read_csv(\"Maumee DL/CrossSectionalData.csv\")\n",
    "print(df_station.shape)\n",
    "print(type(df_station.iloc[12, 1:].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ClimateDatasetV2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = ClimateDatasetV2(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.max(dataset.temporal, dim=0))\n",
    "print(torch.min(dataset.temporal, dim=0))\n",
    "print(torch.std(dataset.temporal, dim=0))\n",
    "print(torch.mean(dataset.temporal, dim=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Compute the max, min, mean, and std using torch functions\n",
    "max_value = torch.max(dataset.stations, dim=0)\n",
    "min_value = torch.min(dataset.stations, dim=0)\n",
    "mean_value = torch.mean(dataset.stations, dim=0)\n",
    "std_value = torch.std(dataset.stations, dim=0)\n",
    "\n",
    "# Create a dictionary to store the values\n",
    "stats_dict = {\n",
    "    'max': max_value.values,\n",
    "    'min': min_value.values,\n",
    "    'mean': mean_value,\n",
    "    'std': std_value\n",
    "}\n",
    "print(\"Max:\", max_value.values.shape)\n",
    "print(\"Min:\", min_value.values)\n",
    "print(\"Mean:\", mean_value)\n",
    "print(\"Std:\", std_value)\n",
    "# Dump the dictionary into a file\n",
    "torch.save(stats_dict, 'trunk_statv2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.max(dataset.targets, dim=0))\n",
    "print(torch.min(dataset.targets, dim=0))\n",
    "print(torch.std(dataset.targets, dim=0))\n",
    "print(torch.mean(dataset.targets, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "a = np.random.randint(0, 100, size=[2000000, 1])\n",
    "for i in tqdm(range(len(a))):\n",
    "    data = a[i].reshape([1,1])\n",
    "    kd = KernelDensity(kernel='gaussian', bandwidth=1).fit(data)  # should be 2D\n",
    "    each_rate = np.exp(kd.score_samples(a))\n",
    "    each_rate /= np.sum(each_rate)  # norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_density_estimation(input_vector, sigma=1):\n",
    "    \"\"\"\n",
    "    Perform Gaussian kernel density estimation on a vector.\n",
    "    \n",
    "    Parameters:\n",
    "    input_vector (numpy.ndarray): Input vector of shape [size, feature_num].\n",
    "    sigma (float): Standard deviation for the Gaussian kernel.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Output vector of shape [size, size].\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(input_vector[:, np.newaxis] - input_vector, axis=2)\n",
    "    output = np.exp(-distances**2 / sigma)\n",
    "    output /= np.sum(output, axis=1, keepdims=True)\n",
    "    \n",
    "    return output\n",
    "a = np.random.randint(0, 100, size=[2000000, 1])\n",
    "gaussian_kernel_density_estimation(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,5],\n",
    "                  [3,4,6]])\n",
    "print(torch.sum(a, dim=1, keepdim=True))\n",
    "t = a / torch.sum(a, dim=1, keepdim=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([0,1,0])\n",
    "a[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(train_features, train_targets, test_features, k):\n",
    "    \"\"\"\n",
    "    Perform k-NN regression using PyTorch tensors.\n",
    "\n",
    "    :param train_features: A tensor of shape (num_train_samples, num_features) containing the training data features.\n",
    "    :param train_targets: A tensor of shape (num_train_samples,) or (num_train_samples, 1) containing the training data targets.\n",
    "    :param test_features: A tensor of shape (num_test_samples, num_features) containing the test data features.\n",
    "    :param k: The number of nearest neighbors to consider for regression.\n",
    "    :return: Predicted targets for the test data.\n",
    "    \"\"\"\n",
    "    # Ensure the input tensors are on the GPU\n",
    "    # train_features = train_features\n",
    "    # train_targets = train_targets.to(\"cuda:0\")\n",
    "    # test_features = test_features.to(\"cuda:0\")\n",
    "\n",
    "    # Calculate the pairwise distances between test points and all training points\n",
    "    distances = torch.cdist(test_features, train_features)\n",
    "\n",
    "    # Determine the indices of the k nearest neighbors (using topk to find the k smallest distances)\n",
    "    _, indices = torch.topk(distances, k, largest=False, sorted=False)\n",
    "\n",
    "    # Gather the targets of the k nearest neighbors\n",
    "    nearest_targets = torch.gather(train_targets.expand(test_features.size(0), -1), 1, indices)\n",
    "\n",
    "    # Calculate the mean target value of the k nearest neighbors\n",
    "    predictions = nearest_targets.mean(dim=1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "import torch\n",
    "\n",
    "# Test case 2: Multiple nearest neighbors\n",
    "train_features = torch.tensor([[1, 2], [3, 4], [5, 6]]).float().cuda()\n",
    "train_targets = torch.tensor([10, 20, 30]).float().cuda()\n",
    "test_features = torch.tensor([[2, 3]]).float().cuda()\n",
    "k = 2\n",
    "expected_output = torch.tensor([15]).float().cuda()\n",
    "output = knn_regression(train_features, train_targets, test_features, k)\n",
    "assert torch.allclose(output, expected_output), f\"Test case 2 failed: Expected {expected_output}, but got {output}\"\n",
    "\n",
    "\n",
    "# Test case 4: Large input tensors\n",
    "train_features = torch.randn(1000, 100)\n",
    "train_targets = torch.randn(1000)\n",
    "test_features = torch.randn(200, 100)\n",
    "k = 5\n",
    "output = knn_regression(train_features, train_targets, test_features, k)\n",
    "assert output.shape == (200,), f\"Test case 4 failed: Expected output shape (200,), but got {output.shape}\"\n",
    "\n",
    "print(\"All test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_density_estimation(input_vector, sigma=1):\n",
    "    \"\"\"\n",
    "    Perform Gaussian kernel density estimation on a tensor.\n",
    "\n",
    "    Parameters:\n",
    "    input_vector (torch.Tensor): Input tensor of shape [size, feature_num].\n",
    "    sigma (float): Standard deviation for the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Output tensor of shape [size, size].\n",
    "    \"\"\"\n",
    "    distances = torch.norm(input_vector.unsqueeze(1) - input_vector, dim=2)\n",
    "    output = torch.exp(-distances**2 / sigma)\n",
    "    output /= torch.sum(output, dim=1, keepdim=True)\n",
    "\n",
    "    return output\n",
    "input_vector = torch.tensor([1,2,3,4]).reshape(4,1).float()\n",
    "print(gaussian_kernel_density_estimation(input_vector)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ClimateDataset\n",
    "dataset = ClimateDataset(\"../Maumee DL/\", split='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch import from_numpy\n",
    "\n",
    "train_temporal, test_temporal = [], []\n",
    "train_targets, test_targets = [], []\n",
    "train_stations, test_stations = [], []\n",
    "df_station = pd.read_csv(os.path.join(\"../Maumee DL\", \"CrossSectionalData.csv\"))\n",
    "\n",
    "for i in range(1, 291):\n",
    "    df = pd.read_csv(os.path.join(\"../Maumee DL\", f\"{i}_climate_sediment.csv\"))\n",
    "    temporal_data = df.iloc[:, 1:7].values\n",
    "    length = temporal_data.shape[0]\n",
    "    train_temporal.append(temporal_data[:int(0.75 * length)])\n",
    "    test_temporal.append(temporal_data[int(0.75 * length):])\n",
    "\n",
    "    target = df.iloc[:, 7].values\n",
    "    train_targets.append(target[:int(0.75 * length)])\n",
    "    test_targets.append(target[int(0.75 * length):])\n",
    "    \n",
    "    station = df_station.iloc[i-1, 1:].values\n",
    "    station = np.repeat(station[np.newaxis, :], temporal_data.shape[0], axis=0)\n",
    "    train_stations.append(station[:int(0.75 * length)])\n",
    "    test_stations.append(station[int(0.75 * length):])\n",
    "\n",
    "train_temporal = from_numpy(np.concatenate(train_temporal, axis=0)).float()\n",
    "test_temporal = from_numpy(np.concatenate(test_temporal, axis=0)).float()\n",
    "train_targets = from_numpy(np.concatenate(train_targets, axis=0)).float()*1000\n",
    "test_targets = from_numpy(np.concatenate(test_targets, axis=0)).float()*1000\n",
    "train_stations = from_numpy(np.concatenate(train_stations, axis=0)).float()\n",
    "test_stations = from_numpy(np.concatenate(test_stations, axis=0)).float()\n",
    "\n",
    "print(train_temporal.shape, test_temporal.shape)\n",
    "print(train_targets.shape, test_targets.shape)\n",
    "print(train_stations.shape, test_stations.shape)\n",
    "\n",
    "# Create a dictionary to store the train and test tensors\n",
    "data_dict = {\n",
    "    'train_stations': train_stations,\n",
    "    'train_temporal': train_temporal,\n",
    "    'train_targets': train_targets,\n",
    "    'test_stations': test_stations,\n",
    "    'test_temporal': test_temporal,\n",
    "    'test_targets': test_targets\n",
    "}\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('../Maumee DL/train_test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f)\n",
    "\n",
    "print(\"Train and test data saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ClimateDatasetV2B\n",
    "dataset = ClimateDatasetV2B(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_dataset import ClimateDatasetV2C\n",
    "from torch.utils.data import DataLoader\n",
    "train = ClimateDatasetV2C(\"../climate_new\", transform=\"MinMaxNormalize\", split=\"train\")\n",
    "stat = train.get_stats()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ClimateDatasetV2C(\"../climate_new\", transform=\"MinMaxNormalize\", split=\"test\", stats=stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size=1024, shuffle=False, pin_memory=True, num_workers=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "station_date = defaultdict(list)\n",
    "from tqdm import tqdm\n",
    "\n",
    "for staids, dates, x, y in tqdm(loader):\n",
    "    x = x[:, -1, :26]\n",
    "    mask = ~(x == -1).any(dim=1)\n",
    "    \n",
    "    for idx, staid in enumerate(staids):\n",
    "        # if staid == '1434021':\n",
    "        #     print(dates[idx], mask[idx].item())\n",
    "        station_date[staid].append((dates[idx], mask[idx].item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for staid, items in station_date.items():\n",
    "    valid_dates = []\n",
    "    for date, mask in items:\n",
    "        if mask:\n",
    "            valid_dates.append(date)\n",
    "    print(staid, len(valid_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35291739978125447\n",
      "Top differences between the two files (ignoring NaN):\n",
      "          Row  Column  Value_in_File1  Value_in_File2    Difference  \\\n",
      "2819      138      40   217112.328125    299988.03125 -82875.703125   \n",
      "2699      132      40   217112.328125    299988.03125 -82875.703125   \n",
      "2779      136      40   217112.328125    299988.03125 -82875.703125   \n",
      "5167      254      40   217112.328125    299988.03125 -82875.703125   \n",
      "2799      137      40   217112.328125    299988.03125 -82875.703125   \n",
      "5107      251      40   217112.328125    299988.03125 -82875.703125   \n",
      "5207      256      40   217112.328125    299988.03125 -82875.703125   \n",
      "2739      134      40   217112.328125    299988.03125 -82875.703125   \n",
      "5227      257      40   217112.328125    299988.03125 -82875.703125   \n",
      "2719      133      40   217112.328125    299988.03125 -82875.703125   \n",
      "2759      135      40   217112.328125    299988.03125 -82875.703125   \n",
      "5127      252      40   217112.328125    299988.03125 -82875.703125   \n",
      "5187      255      40   217112.328125    299988.03125 -82875.703125   \n",
      "5147      253      40   217112.328125    299988.03125 -82875.703125   \n",
      "69095    6365      40   217771.375000    299988.03125 -82216.656250   \n",
      "26307    1870      40   217771.375000    299988.03125 -82216.656250   \n",
      "69075    6364      40   217771.375000    299988.03125 -82216.656250   \n",
      "69135    6367      40   217771.375000    299988.03125 -82216.656250   \n",
      "69155    6368      40   217771.375000    299988.03125 -82216.656250   \n",
      "69055    6363      40   217771.375000    299988.03125 -82216.656250   \n",
      "61196    4255      40   217771.375000    299988.03125 -82216.656250   \n",
      "26267    1868      40   217771.375000    299988.03125 -82216.656250   \n",
      "85339    9399      40   217771.375000    299988.03125 -82216.656250   \n",
      "69035    6362      40   217771.375000    299988.03125 -82216.656250   \n",
      "85279    9396      40   217771.375000    299988.03125 -82216.656250   \n",
      "85319    9398      40   217771.375000    299988.03125 -82216.656250   \n",
      "69115    6366      40   217771.375000    299988.03125 -82216.656250   \n",
      "85239    9394      40   217771.375000    299988.03125 -82216.656250   \n",
      "85259    9395      40   217771.375000    299988.03125 -82216.656250   \n",
      "61277    4259      40   217771.375000    299988.03125 -82216.656250   \n",
      "61216    4256      40   217771.375000    299988.03125 -82216.656250   \n",
      "26387    1874      40   217771.375000    299988.03125 -82216.656250   \n",
      "61236    4257      40   217771.375000    299988.03125 -82216.656250   \n",
      "26367    1873      40   217771.375000    299988.03125 -82216.656250   \n",
      "85299    9397      40   217771.375000    299988.03125 -82216.656250   \n",
      "61257    4258      40   217771.375000    299988.03125 -82216.656250   \n",
      "61297    4260      40   217771.375000    299988.03125 -82216.656250   \n",
      "26347    1872      40   217771.375000    299988.03125 -82216.656250   \n",
      "61317    4261      40   217771.375000    299988.03125 -82216.656250   \n",
      "85219    9393      40   217771.375000    299988.03125 -82216.656250   \n",
      "26327    1871      40   217771.375000    299988.03125 -82216.656250   \n",
      "26287    1869      40   217771.375000    299988.03125 -82216.656250   \n",
      "83748    9279      40   217774.609375    299988.03125 -82213.421875   \n",
      "83768    9280      40   217774.609375    299988.03125 -82213.421875   \n",
      "111008  11745      40   217774.609375    299988.03125 -82213.421875   \n",
      "111029  11746      40   217774.609375    299988.03125 -82213.421875   \n",
      "102565  11325      40   217774.609375    299988.03125 -82213.421875   \n",
      "102585  11326      40   217774.609375    299988.03125 -82213.421875   \n",
      "83707    9277      40   217774.609375    299988.03125 -82213.421875   \n",
      "111049  11747      40   217774.609375    299988.03125 -82213.421875   \n",
      "\n",
      "        Abs_Difference  \n",
      "2819      82875.703125  \n",
      "2699      82875.703125  \n",
      "2779      82875.703125  \n",
      "5167      82875.703125  \n",
      "2799      82875.703125  \n",
      "5107      82875.703125  \n",
      "5207      82875.703125  \n",
      "2739      82875.703125  \n",
      "5227      82875.703125  \n",
      "2719      82875.703125  \n",
      "2759      82875.703125  \n",
      "5127      82875.703125  \n",
      "5187      82875.703125  \n",
      "5147      82875.703125  \n",
      "69095     82216.656250  \n",
      "26307     82216.656250  \n",
      "69075     82216.656250  \n",
      "69135     82216.656250  \n",
      "69155     82216.656250  \n",
      "69055     82216.656250  \n",
      "61196     82216.656250  \n",
      "26267     82216.656250  \n",
      "85339     82216.656250  \n",
      "69035     82216.656250  \n",
      "85279     82216.656250  \n",
      "85319     82216.656250  \n",
      "69115     82216.656250  \n",
      "85239     82216.656250  \n",
      "85259     82216.656250  \n",
      "61277     82216.656250  \n",
      "61216     82216.656250  \n",
      "26387     82216.656250  \n",
      "61236     82216.656250  \n",
      "26367     82216.656250  \n",
      "85299     82216.656250  \n",
      "61257     82216.656250  \n",
      "61297     82216.656250  \n",
      "26347     82216.656250  \n",
      "61317     82216.656250  \n",
      "85219     82216.656250  \n",
      "26327     82216.656250  \n",
      "26287     82216.656250  \n",
      "83748     82213.421875  \n",
      "83768     82213.421875  \n",
      "111008    82213.421875  \n",
      "111029    82213.421875  \n",
      "102565    82213.421875  \n",
      "102585    82213.421875  \n",
      "83707     82213.421875  \n",
      "111049    82213.421875  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df1 = pd.read_csv('../climate_new/01111500.csv', header=None)\n",
    "df2 = pd.read_csv('/home/kh31/Xiaobo/deeponet/climate_new_pgd_0.1x_0.05_0.0125_50/01111500.csv', header=None)\n",
    "\n",
    "# 转换为数值类型，非数值替换为 NaN\n",
    "df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "df2 = df2.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "array1 = df1.to_numpy()\n",
    "array2 = df2.to_numpy()\n",
    "\n",
    "# 忽略 NaN 元素进行比较\n",
    "nan_mask1 = np.isnan(array1)  # 标记 array1 中 NaN 的位置\n",
    "nan_mask2 = np.isnan(array2)  # 标记 array2 中 NaN 的位置\n",
    "nan_mask = nan_mask1 | nan_mask2  # 如果任意一个位置为 NaN，则忽略比较\n",
    "\n",
    "# 比较非 NaN 元素\n",
    "valid_mask = ~nan_mask1 & ~nan_mask2\n",
    "difference_mask = (array1 != array2) & ~nan_mask  # 不相等且不在 NaN 掩码的位置\n",
    "difference_values = np.where(difference_mask, array1 - array2, np.nan)  # 差值（保留 NaN）\n",
    "\n",
    "total_valid_elements = np.sum(valid_mask[:,20:])\n",
    "difference_mask = (array1 != array2) & valid_mask & (abs(array1 - array2) > 0.005)\n",
    "different_elements_count = np.sum(difference_mask)\n",
    "\n",
    "# 计算不同元素的占比\n",
    "difference_ratio = different_elements_count / total_valid_elements\n",
    "print(difference_ratio)\n",
    "# 提取差异数据\n",
    "rows, cols = np.where(difference_mask)  # 找到不相等的元素索引\n",
    "differences = pd.DataFrame({\n",
    "    'Row': rows,\n",
    "    'Column': cols,\n",
    "    'Value_in_File1': array1[rows, cols],\n",
    "    'Value_in_File2': array2[rows, cols],\n",
    "    'Difference': difference_values[rows, cols]\n",
    "})\n",
    "\n",
    "# 按绝对差值排序\n",
    "differences['Abs_Difference'] = differences['Difference'].abs()\n",
    "differences_sorted = differences.sort_values(by='Abs_Difference', ascending=False)\n",
    "\n",
    "# 输出最大的差异\n",
    "print(\"Top differences between the two files (ignoring NaN):\")\n",
    "print(differences_sorted[:50])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
